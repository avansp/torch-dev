<div align="center">

# TORCH-DEV

[![python](https://img.shields.io/badge/-Python_3.12-blue?logo=python&logoColor=white)]()
[![pytorch](https://img.shields.io/badge/PyTorch_2.0+-ee4c2c?logo=pytorch&logoColor=white)](https://pytorch.org/)
[![lightning](https://img.shields.io/badge/-Lightning_2.0+-792ee5?logo=pytorchlightning&logoColor=white)](https://lightning.ai/docs/pytorch/stable/)
[![hydra](https://img.shields.io/badge/Config-Hydra_1.3-89b8cd)](https://hydra.cc/)

</div>

`TORCH-DEV` is a template to create a pyTorch structure using [Lightning](https://github.com/Lightning-AI/pytorch-lightning) and [Hydra](https://github.com/facebookresearch/hydra) framework, simplified from [Ashleve's Lightning-Hydra-Template](https://github.com/ashleve/lightning-hydra-template/tree/main).

<div align="center">

[![Button Icon]](https://github.com/ashleve/lightning-hydra-template/generate)

</div>

## Installation

```{bash}
pip install -r requirements.txt
```

Install the package script (note use `-e` for development)
```{bash}
pip install .
```

## Training

An example of MNIST classification is provided for training.

```{bash}
train task=mnist
```

<details>
<summary><b>Show how the training works</b></summary>
<br>

1. Starting from the main configuration [`configs/train.yaml`](src/configs/train.yaml) file, it assigns the `task=???` variable to read the configuration from [`configs/task/mnist.yaml`](src/configs/task/mnist.yaml) file, which further:

    * sets `model=image_classifier`,

    * defines a data module instance from [`MNISTDataModule` class](src/modules/data/mnist_datamodule.py), and

    * customises early_stopping and best_model_checkpoint callbacks.

1. Instantiate a [LightningModule](https://lightning.ai/docs/pytorch/stable/common/lightning_module.html) object as an instance from [`ImageClassifierLitModule` class](src/modules/model/image_classifier.py).

1. Instantiate a [LightningDataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html) object as an instance from [`MNISTDataModule` class](src/modules/data/mnist_datamodule.py).

1. Instantitate callbacks defined in [`configs/callbacks`](src/configs/callbacks/) folder.

1. Instantiate loggers defined in [`configs/logger`](src/configs/logger/) folder.

1. Instantiate the default [Trainer](https://lightning.ai/docs/pytorch/stable/common/trainer.html) as defined in [`configs/trainer/default.yaml`](src/configs/trainer/default.yaml) file.

1. Run the Trainer's [`fit()`](https://lightning.ai/docs/pytorch/stable/common/trainer.html#fit) function.

</details>

<details>
<summary><b>Show model config for MNIST classification</b></summary>
<br>

```yaml
# specific image dimension for the MNIST images
model:
  width: 28
  height: 28
  channel: 1
  num_classes: 10
```
</details>

<details>
<summary><b>Show MNIST data config</b></summary>

<br>

MNIST data is defined by [MNISTDataModule](src/modules/data/mnist_datamodule.py) class, and instantiated using a config file in [mnist.yaml](src/configs/task/mnist.yaml) file, as follows:

```yaml
data:
  _target_: src.modules.data.mnist_datamodule.MNISTDataModule
  data_dir: ${paths.data_dir}
  batch_size: 128 
  train_pct: 0.7
  num_workers: 0
  pin_memory: False
```

</details>


<details>
<summary><b>Show output directory</b></summary>

<br>

The path of the output is automatically generated by the script and it is shown in the console log.

```
output directory
├── checkpoints           <- Model checkpoints
│   ├── best_model.ckpt   <- Link to the best model file
│   ├── best_metric.csv   <- Train result metrics from the best model
│   ├── last.ckpt         <- Link to the last model file
│   ├── last_metric.csv   <- Train result metrics from the last checkpoint model
│   ├── ....ckpt          <- Saved model file
│   └── ....ckpt
│
├── config_tree.log       <- The configuration printed as a tree
├── config.yaml           <- Complete configuration
├── train.log             <- Console log
│
├── csv                   <- Logs from CSV callback
│   └── version_0
│       ├── hparams.yaml
│       └── metrics.csv
└── tensorboard           <- Logs from Tensorboard
    └── version_0
        ├── ...
        └── ...
```

</details>


## Evaluate 

Evaluate the training performance. See all the available arguments using `--help` option:

```console
evaluate --help
```

## Multi-run mode

Hydra allows `MULTIRUN` mode to run several training jobs sequentially. This can be useful for sweeping parameters (*hyperparameter searching*) or to do a cross-validation experiment. See https://hydra.cc/docs/tutorials/basic/running_your_app/multi-run/.

<details>
<summary><b>Hyper-parameter searching with Optuna sweeper plugin</b></summary>

<br>

Hyper-parameter searching can be performed by running the training process multiple times in the `MULTIRUN` mode. You can create a hyper-parameter searching config file under the `mruns` folder. An example is given in the [mnist_hparams.yaml](src/configs/mruns/mnist_hparams.yaml) config file using [Optuna sweeper plugin](https://hydra.cc/docs/plugins/optuna_sweeper/).

```console
train task=mnist mruns=mnist_hparams
```

An important part of that configuration file is the sweeping parameters, which define how to search hyperparameter values:
```yaml
# define hyperparameter search space
params:
  model.optimizer.lr: interval(0.0001, 0.1)
  data.batch_size: choice(32, 64, 128, 256)
  model.net.lin1_size: choice(64, 128, 256)
  model.net.lin2_size: choice(64, 128, 256)
  model.net.lin3_size: choice(32, 64, 128, 256)
```

</details>

## How to's

<details>
<summary><b>How to run sanity check training</b></summary>

<br>

Use the `sanity_check.yaml` configuration for the trainer:

```console
train task=mnist trainer=sanity_check
```

You can also test to run the training for 5 training steps:

```console
train task=mnist trainer=sanity_check trainer.fast_dev_run=5
```

Note that the fast_dev_run flag will disable all loggers and some callbacks. See https://lightning.ai/docs/pytorch/stable/common/trainer.html#fast-dev-run.

</details>

<details>
<summary><b>How to resume training</b></summary>
<br>

Use the last checkpoint to start the new training. E.g:

```console
train task=mnist ckpt_path=outputs/mnist/runs/2026-01-08_10-03-47/checkpoints/last.ckpt
```

</details>

<details>
<summary><b>How to add a new logger</b></summary>
<br>

The default loggers use csv and tensorboard. Let's say you want to use another alternative logger for your training from [PyTorch Lightning's loggers](https://lightning.ai/docs/pytorch/stable/api_references.html#loggers). For example, the [MLFlowLogger](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.mlflow.html#module-lightning.pytorch.loggers.mlflow) from [MLflow](https://mlflow.org/).

You can easily add this in the [logger's default list](src/configs/logger/default.yaml) as follows:

```yaml
defaults:
  - csv
  - tensorboard
  - mlflow
```

and create a new `mlflow.yaml` configuration as follows:

```yaml
mlflow:
  _target_: lightning.pytorch.loggers.mlflow.MLFlowLogger
  experiment_name: ${name}
  run_name: ""
  tracking_uri: ${paths.log_dir}/mlflow/ 
  tags: null
  prefix: ""
  artifact_location: null
```

After training, go to `outputs/mlflow` folder and run MLFlow

```console
mlflow server --host 127.0.0.1 --port 8585
```

and open it from http://127.0.0.1:8585.

</details>

[Button Icon]: https://img.shields.io/badge/USE_THIS_TEMPLATE-orange?style=for-the-badge
